{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83e8e55-c147-4136-bdf2-5cc652526cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96eda1d-98d3-453f-abad-d17122830e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = 'sarcasm_training (1).csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f444ab3-81b9-4e29-80da-be8a1cbdce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset after filtering meaningless words:\n",
      "                                                text  labels  \\\n",
      "0  <user> thanks for showing up for our appointme...       1   \n",
      "1                                      haha .  # lol       1   \n",
      "2  i love waiting <num> min for a cab - such shor...       1   \n",
      "3  22 super funny quotes # funnyquotes  # funnysa...       1   \n",
      "4            goog morning  # sorrynotsorry # morning       1   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0              user thanks showing appointment today  \n",
      "1                                           haha lol  \n",
      "2  love waiting num min cab shortage user please ...  \n",
      "3  super funny quotes funnyquotes funnysayings hi...  \n",
      "4                      morning sorrynotsorry morning  \n",
      "Training and testing sets prepared.\n",
      "One-Hot Encoded shape: (15568, 19214)\n",
      "Label Encoded shape: (15568, 1)\n",
      "TF-IDF Encoded shape: (15568, 5000)\n",
      "Word2Vec Encoded shape: (15568, 100)\n",
      "Term Frequency Encoded shape: (15568, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset after filtering meaningless words\n",
    "print(\"First few rows of the dataset after filtering meaningless words:\")\n",
    "print(df.head())\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "X_onehot = onehot_encoder.fit_transform(df['cleaned_text'].values.reshape(-1, 1))\n",
    "\n",
    "# 2. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "X_label = label_encoder.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# 3. TF-IDF Encoding\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "# 4. Word2Vec\n",
    "sentences = df['cleaned_text'].apply(word_tokenize).tolist()\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create a Word2Vec representation for each comment by averaging the word vectors\n",
    "def get_word2vec_embedding(text):\n",
    "    words = text.split()\n",
    "    word_vecs = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(100)\n",
    "\n",
    "X_word2vec = np.array([get_word2vec_embedding(text) for text in df['cleaned_text']])\n",
    "\n",
    "# 5. Term Frequency Encoding\n",
    "count_vectorizer = CountVectorizer(max_features=5000)\n",
    "X_term_freq = count_vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
    "\n",
    "# Splitting Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_onehot, X_test_onehot, y_train, y_test = train_test_split(X_onehot, df['labels'].values, test_size=0.2, random_state=42)\n",
    "X_train_label, X_test_label, _, _ = train_test_split(X_label.reshape(-1, 1), df['labels'].values, test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(X_tfidf, df['labels'].values, test_size=0.2, random_state=42)\n",
    "X_train_word2vec, X_test_word2vec, _, _ = train_test_split(X_word2vec, df['labels'].values, test_size=0.2, random_state=42)\n",
    "X_train_term_freq, X_test_term_freq, _, _ = train_test_split(X_term_freq, df['labels'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training and testing sets prepared.\")\n",
    "\n",
    "# Print shapes of the encoded datasets to verify\n",
    "print(\"One-Hot Encoded shape:\", X_train_onehot.shape)\n",
    "print(\"Label Encoded shape:\", X_train_label.shape)\n",
    "print(\"TF-IDF Encoded shape:\", X_train_tfidf.shape)\n",
    "print(\"Word2Vec Encoded shape:\", X_train_word2vec.shape)\n",
    "print(\"Term Frequency Encoded shape:\", X_train_term_freq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ecc775-d955-413a-a17a-6c1e1468ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\91636\\anaconda3\\lib\\site-packages (1.24.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\91636\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91636\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\91636\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\91636\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from gensim) (2.0.9)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: pyfume in c:\\users\\91636\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\91636\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\91636\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\91636\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: simpful==2.12.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso==1.8.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\91636\\anaconda3\\lib\\site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas scikit-learn gensim tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac868461-0674-4d4b-bfa0-b111ea8b0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sarcasm_training (1).csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6eb23-05d7-484b-bf00-4fa57fa5ca8f",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding transforms each word in the text into a binary vector of length equal to the size of the vocabulary. Each word is represented by a vector where only the index corresponding to that word is set to 1, and all other indices are set to 0. This method is useful for categorical text data but can result in high-dimensional sparse matrices, which may require substantial memory for large vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5a4383-5eb0-4f1f-9de6-6056fce2e5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91636\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_index.py:103: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding Results:\n",
      "Test Accuracy: 71.37%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76      2210\n",
      "           1       0.71      0.60      0.65      1754\n",
      "\n",
      "    accuracy                           0.71      3964\n",
      "   macro avg       0.71      0.70      0.70      3964\n",
      "weighted avg       0.71      0.71      0.71      3964\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1776  434]\n",
      " [ 701 1053]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define a function to train and evaluate a Random Forest classifier\n",
    "def train_rf(X_train, X_test, y_train, y_test):\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Tokenize the text data with a limited vocabulary size\n",
    "max_words = 1000  # Further limit vocabulary to the top 1000 words\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['cleaned_comment'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_comment'])\n",
    "\n",
    "# Create a sparse one-hot encoded matrix\n",
    "onehot_results_sparse = csr_matrix((len(sequences), max_words), dtype=np.float32)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for word_index in seq:\n",
    "        if word_index < max_words:\n",
    "            onehot_results_sparse[i, word_index] = 1\n",
    "\n",
    "# Split data for One Hot Encoding\n",
    "X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe = train_test_split(onehot_results_sparse, df['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert sparse matrices to dense format before fitting the classifier (as RandomForestClassifier does not accept sparse matrices)\n",
    "X_train_ohe = X_train_ohe.toarray()\n",
    "X_test_ohe = X_test_ohe.toarray()\n",
    "\n",
    "# Define and train the Random Forest classifier for One Hot Encoding\n",
    "print(\"One Hot Encoding Results:\")\n",
    "train_rf(X_train_ohe, X_test_ohe, y_train_ohe, y_test_ohe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f6d1c-4f85-403c-a3b0-e8d84545f6ad",
   "metadata": {},
   "source": [
    "## TF-IDF Encoding\n",
    "\n",
    "TF-IDF encoding combines Term Frequency (TF) with Inverse Document Frequency (IDF), which measures how unique or rare a term is across a collection of documents. This technique helps in highlighting words that are important to specific documents while down-weighting common terms that appear frequently across many documents. It balances the term's local importance with its global significance, making it a robust method for text representation in natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95453479-c164-4b1a-8c30-e9d96acf4f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Encoding Results:\n",
      "Test Accuracy: 72.75%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78      2210\n",
      "           1       0.75      0.58      0.65      1754\n",
      "\n",
      "    accuracy                           0.73      3964\n",
      "   macro avg       0.73      0.71      0.71      3964\n",
      "weighted avg       0.73      0.73      0.72      3964\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1875  335]\n",
      " [ 745 1009]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data for TF-IDF encoding\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(df['cleaned_comment'], df['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TF-IDF Encoding\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test_text)\n",
    "\n",
    "# Define and train the Random Forest classifier for TF-IDF\n",
    "def train_rf(X_train, X_test, y_train, y_test):\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# TF-IDF Encoding Results\n",
    "print(\"TF-IDF Encoding Results:\")\n",
    "train_rf(X_train_tfidf, X_test_tfidf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e51ff56-5197-4c22-a5b6-8f05f6d1a973",
   "metadata": {},
   "source": [
    "## Term Frequency (TF) Encoding\n",
    "Term Frequency (TF) encoding represents the frequency of each word in a document. It captures how often a term appears in a document relative to the total number of words in that document. This method is simple and effective for understanding the distribution of terms within a text, but it does not account for the importance of terms across different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac85e357-fb98-40e3-8f34-605cfd2b98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Encoding Results:\n",
      "Test Accuracy: 72.35%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77      2210\n",
      "           1       0.75      0.56      0.64      1754\n",
      "\n",
      "    accuracy                           0.72      3964\n",
      "   macro avg       0.73      0.71      0.71      3964\n",
      "weighted avg       0.73      0.72      0.72      3964\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1881  329]\n",
      " [ 767  987]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split data for Term Frequency encoding\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(df['cleaned_comment'], df['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Term Frequency Encoding\n",
    "vectorizer_tf = CountVectorizer(max_features=5000)\n",
    "X_train_tf = vectorizer_tf.fit_transform(X_train_text)\n",
    "X_test_tf = vectorizer_tf.transform(X_test_text)\n",
    "\n",
    "# Define and train the Random Forest classifier for Term Frequency\n",
    "print(\"Term Frequency Encoding Results:\")\n",
    "train_rf(X_train_tf, X_test_tf, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5808e40-4575-488e-988f-7f416405c83f",
   "metadata": {},
   "source": [
    "## Word2Vec Encoding\n",
    "Word2Vec is a powerful word embedding technique that transforms words into continuous vector representations based on their context within a text corpus. This method enables the encoding of syntactic and semantic similarities, enhancing the performance of various natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594c3b47-ef90-491f-90c5-d04ac0e2610c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Encoding Results:\n",
      "Test Accuracy: 65.11%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.79      0.72      2210\n",
      "           1       0.65      0.47      0.54      1754\n",
      "\n",
      "    accuracy                           0.65      3964\n",
      "   macro avg       0.65      0.63      0.63      3964\n",
      "weighted avg       0.65      0.65      0.64      3964\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1756  454]\n",
      " [ 929  825]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize comments\n",
    "df['tokenized_comment'] = df['cleaned_comment'].apply(lambda x: x.split())\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=df['tokenized_comment'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Helper function to get average Word2Vec embeddings\n",
    "def get_avg_word2vec(tokens_list, model, vector_size):\n",
    "    vec = np.zeros(vector_size).reshape((1, vector_size))\n",
    "    count = 0\n",
    "    for word in tokens_list:\n",
    "        try:\n",
    "            vec += model.wv[word].reshape((1, vector_size))\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "# Get Word2Vec embeddings for each comment\n",
    "X_word2vec = np.concatenate([get_avg_word2vec(comment, w2v_model, 100) for comment in df['tokenized_comment']], axis=0)\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_word2vec, df['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define and train the Random Forest classifier for Word2Vec\n",
    "print(\"Word2Vec Encoding Results:\")\n",
    "train_rf(X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af7886-f354-4d72-bebf-fdeeca590705",
   "metadata": {},
   "source": [
    "### Optional(practice purposes only)\n",
    "[Model Compilation and Training]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d0492-9e6e-4e39-8e54-f6078845a9ea",
   "metadata": {},
   "source": [
    "#### Tokenization: \n",
    "Converting text to sequences of integers.\n",
    "\n",
    "#### Padding: \n",
    "Ensuring all sequences are of the same length.\n",
    "\n",
    "#### Embedding Layer: \n",
    "Transforming words into dense vectors.\n",
    "\n",
    "#### LSTM Layer: \n",
    "Capturing temporal patterns in the text.\n",
    "\n",
    "#### Dropout Layer:\n",
    "Preventing overfitting.\n",
    "\n",
    "#### Model Compilation and Training:\n",
    "Preparing and training the model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "223a642a-bf87-466a-8f3e-40a5ab919ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'labels'], dtype='object')\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91636\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 84ms/step - accuracy: 0.6764 - loss: 0.5743 - val_accuracy: 0.8146 - val_loss: 0.4069\n",
      "Epoch 2/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 81ms/step - accuracy: 0.9121 - loss: 0.2273 - val_accuracy: 0.8052 - val_loss: 0.4566\n",
      "Epoch 3/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 81ms/step - accuracy: 0.9661 - loss: 0.0941 - val_accuracy: 0.7982 - val_loss: 0.5612\n",
      "Epoch 4/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 84ms/step - accuracy: 0.9818 - loss: 0.0540 - val_accuracy: 0.7926 - val_loss: 0.7086\n",
      "Epoch 5/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 84ms/step - accuracy: 0.9878 - loss: 0.0347 - val_accuracy: 0.7856 - val_loss: 0.9342\n",
      "Epoch 6/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 95ms/step - accuracy: 0.9897 - loss: 0.0301 - val_accuracy: 0.7957 - val_loss: 0.9730\n",
      "Epoch 7/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - accuracy: 0.9912 - loss: 0.0212 - val_accuracy: 0.7888 - val_loss: 0.9851\n",
      "Epoch 8/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 89ms/step - accuracy: 0.9938 - loss: 0.0177 - val_accuracy: 0.7907 - val_loss: 1.0542\n",
      "Epoch 9/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 95ms/step - accuracy: 0.9932 - loss: 0.0161 - val_accuracy: 0.7926 - val_loss: 1.1938\n",
      "Epoch 10/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 94ms/step - accuracy: 0.9963 - loss: 0.0100 - val_accuracy: 0.7768 - val_loss: 1.1172\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.7880 - loss: 1.0571\n",
      "Test Accuracy: 77.65%\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('sarcasm_training (1).csv')\n",
    "\n",
    "# Verify column names\n",
    "print(df.columns)\n",
    "\n",
    "# Assuming 'cleaned_comment' exists, continue with tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Encode labels\n",
    "y = to_categorical(df['labels'])\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "                    output_dim=128,\n",
    "                    input_length=max_sequence_length))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))  # 2 classes: sarcasm or not\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Example predictions (for demonstration purposes)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9f476-be61-4bac-bc8b-ba943a0e35d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
