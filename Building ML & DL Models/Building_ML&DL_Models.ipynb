{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5va_KOkfjR5"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L-A7DHwUJgn",
        "outputId": "241555db-b753-43fb-effb-435e6d4965b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDnklkmoU7Kz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/preprocessed_dataset.csv')\n",
        "\n",
        "# Define the text and target columns\n",
        "text_column = 'cleaned_comment'\n",
        "target_column = 'labels'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnAjQxA5f-yv"
      },
      "source": [
        "### Import Necessary Libraries\n",
        "\n",
        "- **nltk**: Natural Language Toolkit, used for various text processing tasks.\n",
        "- **re**: Regular expressions, used for string manipulation.\n",
        "\n",
        "### Download NLTK Resources\n",
        "\n",
        "- **stopwords**: A list of common words that are typically filtered out in text processing.\n",
        "- **punkt**: Tokenizer models, used for breaking text into words and sentences.\n",
        "\n",
        "### Define the preprocess_text Function\n",
        "\n",
        "1. Convert text to lowercase.\n",
        "2. Remove URLs using regular expressions.\n",
        "3. Remove punctuation and digits using regular expressions.\n",
        "4. Remove stopwords using the NLTK stopwords list.\n",
        "5. Tokenize the text using NLTK's `word_tokenize` function.\n",
        "6. Filter out stopwords from the tokens.\n",
        "7. Join the filtered tokens back into a single string.\n",
        "\n",
        "### Apply Text Preprocessing\n",
        "\n",
        "- Apply the `preprocess_text` function to a specified column (`text_column`) in a DataFrame (`df`).\n",
        "\n",
        "### Output\n",
        "\n",
        "The output of this code is a DataFrame column where each text entry has been cleaned and preprocessed according to the steps defined in the `preprocess_text` function. The text in this column will be:\n",
        "\n",
        "- Converted to lowercase.\n",
        "- Stripped of URLs, punctuation, and digits.\n",
        "- Filtered to remove common stopwords.\n",
        "- Tokenized and then rejoined into a single string of meaningful words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS7GOnw8Vy3s",
        "outputId": "94b0520f-066a-42a2-b403-96cb988133bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Jxd2OCVFgE",
        "outputId": "c4f3c99a-a6df-45e0-9fde-4c996c6bd270"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a function to clean and preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a single string\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing\n",
        "df[text_column] = df[text_column].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJURwCFgEuL"
      },
      "source": [
        "## Vectorize the Text Data Using TF-IDF\n",
        "\n",
        "### Import Necessary Library\n",
        "\n",
        "- **sklearn.feature_extraction.text.TfidfVectorizer**: A tool from the `scikit-learn` library used for transforming text data into TF-IDF (Term Frequency-Inverse Document Frequency) features.\n",
        "\n",
        "### Vectorize the Text Data Using TF-IDF\n",
        "\n",
        "1. **Initialize TF-IDF Vectorizer**:\n",
        "    - Create an instance of `TfidfVectorizer` with a maximum feature limit of 5000. This means the vectorizer will consider only the top 5000 terms based on their TF-IDF scores.\n",
        "\n",
        "2. **Fit and Transform the Text Data**:\n",
        "    - Apply the `fit_transform` method on the text data (`df[text_column]`). This step involves learning the vocabulary from the text data and then transforming the text into a TF-IDF feature matrix.\n",
        "    - The result, `X`, is a sparse matrix where each row represents a document (or text entry) and each column represents a term (word) from the vocabulary. The values in the matrix are the TF-IDF scores.\n",
        "\n",
        "3. **Assign Target Variable**:\n",
        "    - Assign the target variable column from the DataFrame (`df[target_column]`) to `y`.\n",
        "\n",
        "### Output\n",
        "\n",
        "The output of this code is:\n",
        "\n",
        "- `X`: A sparse matrix of shape `(n_samples, max_features)` where `n_samples` is the number of documents (text entries) in the DataFrame and `max_features` is 5000. Each element in the matrix represents the TF-IDF score of a term in a document.\n",
        "- `y`: A series or array containing the target variable values from the specified column in the DataFrame.\n",
        "\n",
        "For example, if the text data in `df[text_column]` contains three documents:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnagGdzlVKP0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf_vectorizer.fit_transform(df[text_column])\n",
        "y = df[target_column]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssDE1zZgaRY"
      },
      "source": [
        "### Handling Imbalanced Data for Machine Learning Models\n",
        "\n",
        "### Apply SMOTE to the TF-IDF Vectorized Data\n",
        "\n",
        "1. **Initialize SMOTE**:\n",
        "    - Create an instance of the `SMOTE` class with a specified `random_state` for reproducibility.\n",
        "\n",
        "2. **Resample the Data**:\n",
        "    - Apply the `fit_resample` method to the TF-IDF vectorized data (`X_tfidf`) and the target variable (`y`). This step involves generating synthetic samples to balance the class distribution.\n",
        "    - The result is two new arrays: `X_resampled` (the resampled feature matrix) and `y_resampled` (the resampled target variable).\n",
        "\n",
        "### Split the Data into Training and Testing Sets\n",
        "\n",
        "1. **Split the Data**:\n",
        "    - Use the `train_test_split` function to split the resampled data into training and testing sets.\n",
        "    - Specify the `test_size` parameter to determine the proportion of the dataset to include in the test split (20% in this case).\n",
        "    - Set the `random_state` for reproducibility.\n",
        "\n",
        "### Output\n",
        "\n",
        "The output of this code is:\n",
        "\n",
        "- **X_resampled, y_resampled**: The feature matrix and target variable after applying SMOTE to balance the class distribution.\n",
        "- **X_train, X_test, y_train, y_test**: The training and testing sets derived from the resampled data.\n",
        "\n",
        "For example, if the original dataset had a severe class imbalance, SMOTE would generate synthetic samples to balance the classes, resulting in `X_resampled` and `y_resampled`. The `train_test_split` function then splits this balanced data into training and testing sets, ensuring that the training set (`X_train`, `y_train`) can be used to train a machine learning model, and the testing set (`X_test`, `y_test`) can be used to evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqPSCBJRggDJ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Apply SMOTE to the TF-IDF vectorized data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uh-SGGxgiyJ"
      },
      "source": [
        "##  Function to Build and Evaluate Machine Learning Models\n",
        "\n",
        "\n",
        "\n",
        "### Define the build_and_evaluate_ml_model Function\n",
        "\n",
        "This function builds and evaluates a machine learning model based on the specified type.\n",
        "\n",
        "1. **Select Model Type**:\n",
        "    - `model_type` can be one of the following:\n",
        "      - `'random_forest'`: Uses `RandomForestClassifier`.\n",
        "      - `'logistic_regression'`: Uses `LogisticRegression`.\n",
        "      - `'svm'`: Uses `SVC`.\n",
        "    - An error is raised if an unsupported model type is provided.\n",
        "\n",
        "2. **Train the Model**:\n",
        "    - The model is instantiated with `random_state=42` for reproducibility.\n",
        "    - The model is trained using the `fit` method with the training data (`X_train`, `y_train`).\n",
        "\n",
        "3. **Make Predictions**:\n",
        "    - The trained model makes predictions on the test data (`X_test`) using the `predict` method.\n",
        "\n",
        "4. **Evaluate the Model**:\n",
        "    - The model's performance is evaluated using the `accuracy_score` and `classification_report` functions.\n",
        "    - The accuracy score and classification report are printed, providing a detailed assessment of the model's performance.\n",
        "\n",
        "5. **Return the Model**:\n",
        "    - The trained model is returned for further use if needed.\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "To use the function, you can call it with different model types as shown in the example:\n",
        "\n",
        "```python\n",
        "# Example usage\n",
        "rf_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest')\n",
        "lr_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='logistic_regression')\n",
        "svm_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='svm')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdKhKoXUWOhw"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest'):\n",
        "    if model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "    elif model_type == 'logistic_regression':\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    elif model_type == 'svm':\n",
        "        model = SVC(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type. Choose from 'random_forest', 'logistic_regression', or 'svm'.\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Model: {model_type}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model\n",
        "\n",
        "# # Example usage\n",
        "# rf_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='random_forest')\n",
        "# lr_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='logistic_regression')\n",
        "# svm_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='svm')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTB4bdjUgtXp"
      },
      "source": [
        "## Handling Imbalanced Data for Deep Learning Models\n",
        "\n",
        "\n",
        "### Tokenization, Padding, and Handling Imbalanced Data for Deep Learning\n",
        "\n",
        "Here, we are preparing text data for deep learning models by tokenizing, padding sequences, and handling class imbalance using SMOTE.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Tokenization**:\n",
        "   - Using `Tokenizer` from `TensorFlow.keras.preprocessing.text`, we convert text data (`df[text_column]`) into sequences of integers. We limit the vocabulary size to 5000 words (`num_words=5000`).\n",
        "\n",
        "2. **Padding Sequences**:\n",
        "   - `pad_sequences` from `TensorFlow.keras.preprocessing.sequence` is used to ensure all sequences have the same length (`maxlen=100`). Sequences are padded with zeros (`padding='post'`) at the end.\n",
        "\n",
        "3. **Handling Imbalance with SMOTE**:\n",
        "   - `SMOTE` from `imblearn.over_sampling` is applied to the padded sequences (`X_padded`) and target variable (`df[target_column]`) to balance the class distribution.\n",
        "\n",
        "4. **Splitting into Training and Testing Sets**:\n",
        "   - `train_test_split` from `sklearn.model_selection` is used to split the balanced data (`X_resampled_dl`, `y_resampled_dl`) into training (`X_train_dl`, `y_train_dl`) and testing (`X_test_dl`, `y_test_dl`) sets.\n",
        "\n",
        "### Output\n",
        "\n",
        "After executing this code, you will have:\n",
        "- `X_train_dl`, `X_test_dl`: Padded sequences ready for training and testing deep learning models.\n",
        "- `y_train_dl`, `y_test_dl`: Corresponding target variables (labels) for training and testing.\n",
        "  \n",
        "This preprocessing prepares the text data by converting it into numerical sequences, ensuring uniform sequence lengths, and addressing class imbalance for use in deep learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jY1NBfFX38Y"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df[text_column])\n",
        "X_tokenized = tokenizer.texts_to_sequences(df[text_column])\n",
        "\n",
        "# Pad the sequences\n",
        "maxlen = 100\n",
        "X_padded = pad_sequences(X_tokenized, padding='post', maxlen=maxlen)\n",
        "\n",
        "# Apply SMOTE to the padded sequences\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled_dl, y_resampled_dl = smote.fit_resample(X_padded, df[target_column])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_resampled_dl, y_resampled_dl, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDb8DJE4WJjx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[text_column])\n",
        "y = df[target_column]\n",
        "\n",
        "# Apply SMOTE to the data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsuSpf0RhFay"
      },
      "source": [
        "## Function to Build and Evaluate Deep Learning Models\n",
        "\n",
        "### Building and Evaluating a Deep Learning Model\n",
        "\n",
        "This code snippet demonstrates how to build and evaluate a simple deep learning model using TensorFlow and Keras for text classification.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Label Encoding**:\n",
        "   - Convert the target variable (`y_train` and `y_test`) to categorical labels using `LabelEncoder` from `sklearn.preprocessing`. This step is necessary if the target variable is not already encoded.\n",
        "\n",
        "2. **Define the Model Architecture**:\n",
        "   - Use `Sequential` from `tensorflow.keras.models` to define a sequential model.\n",
        "   - Add an `Embedding` layer with an input dimension of 5000 (vocabulary size), output dimension of 16 (embedding dimension), and input length (`maxlen`) determined during padding.\n",
        "   - Add a `GlobalAveragePooling1D` layer to pool the embeddings across the sequence dimension.\n",
        "   - Add `Dense` layers with 24 units and ReLU activation, followed by a final `Dense` layer with 1 unit and sigmoid activation for binary classification.\n",
        "\n",
        "3. **Compile the Model**:\n",
        "   - Compile the model using `'adam'` optimizer and `'binary_crossentropy'` loss function for binary classification. Metrics are set to `'accuracy'` for evaluation.\n",
        "\n",
        "4. **Train the Model**:\n",
        "   - Train the model on `X_train` and `y_train` with 10 epochs, a batch size of 32, and a validation split of 0.2 (20% of training data used for validation).\n",
        "\n",
        "5. **Evaluate the Model**:\n",
        "   - Evaluate the trained model on `X_test` and `y_test` to measure its performance in terms of accuracy.\n",
        "\n",
        "### Output\n",
        "\n",
        "After executing `build_and_evaluate_dl_model`, you will see the following output:\n",
        "- The accuracy of the deep learning model on the test set (`X_test`, `y_test`).\n",
        "\n",
        "This function encapsulates the process of building, training, and evaluating a deep learning model for text classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wc5nCdcWSIH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def build_and_evaluate_dl_model(X_train, X_test, y_train, y_test):\n",
        "    # Convert target to categorical if necessary\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=16, input_length=maxlen),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Deep Learning Model Accuracy: {accuracy}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "#dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_YL6czyhM9R"
      },
      "source": [
        " ### Function Calls to Generate Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "- The SVM model outperforms both the Random Forest and Logistic Regression models in terms of accuracy and F1-score.\n",
        "- SVM shows a balanced performance with good precision and recall for both classes (0 and 1), indicating robust classification capabilities.\n",
        "- Random Forest and Logistic Regression models also perform reasonably well, but SVM provides slightly better performance metrics across accuracy, precision, recall, and F1-score.\n",
        "  \n",
        "In summary, based on this evaluation, the SVM model is recommended for this classification task due to its superior performance metrics compared to Random Forest and Logistic Regression models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YhRHoyD9lRze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdgJntSGWbhK",
        "outputId": "51374f34-0ae7-4275-eb23-401909f998de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: random_forest\n",
            "Accuracy: 0.7660924750679964\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77      2189\n",
            "           1       0.78      0.75      0.76      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: logistic_regression\n",
            "Accuracy: 0.772438803263826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.77      0.77      2189\n",
            "           1       0.78      0.77      0.77      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: svm\n",
            "Accuracy: 0.8125566636446057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82      2189\n",
            "           1       0.84      0.78      0.81      2223\n",
            "\n",
            "    accuracy                           0.81      4412\n",
            "   macro avg       0.81      0.81      0.81      4412\n",
            "weighted avg       0.81      0.81      0.81      4412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# For Machine Learning models\n",
        "rf_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest')\n",
        "lr_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='logistic_regression')\n",
        "svm_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='svm')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning and Model Evaluation\n",
        "\n",
        "The provided code snippet performs hyperparameter tuning for the SVM model using GridSearchCV and evaluates its performance on the test set.\n",
        "\n",
        "#### Steps:\n",
        "\n",
        "1. **Hyperparameter Tuning with GridSearchCV**:\n",
        "   - GridSearchCV is used to search for the best combination of hyperparameters (`C`, `gamma`, `kernel`) for the SVM model (`SVC(random_state=42)`).\n",
        "   - The parameter grid (`svm_param_grid`) specifies different values for `C`, `gamma`, and `kernel` to be evaluated.\n",
        "   - Cross-validation (`cv=3`) with 3 folds is used to validate the performance of each parameter combination.\n",
        "   - The best performing model based on accuracy is selected (`best_model = grid_search.best_estimator_`).\n",
        "\n",
        "2. **Model Evaluation**:\n",
        "   - The best model obtained from GridSearchCV is evaluated on the test set (`X_test`, `y_test`).\n",
        "   - Accuracy, precision, recall, and F1-score are computed and printed using `classification_report`.\n",
        "\n",
        "#### Output:\n",
        "\n",
        "The output shows the evaluation metrics of the best SVM model on the test set:\n",
        "\n",
        "- **Accuracy:** 0.811\n",
        "- **Precision (0/1):** 0.81 / 0.81\n",
        "- **Recall (0/1):** 0.81 / 0.81\n",
        "- **F1-score (0/1):** 0.81 / 0.81\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- The tuned SVM model achieves a balanced accuracy, precision, recall, and F1-score of around 0.81 for both classes (0 and 1).\n",
        "- This indicates that the SVM model with the best hyperparameters performs well in classifying the data, demonstrating robustness and generalization capability.\n",
        "- Further hyperparameter tuning and evaluation for Gradient Boosting and XGBoost models (commented out in the provided code) can be similarly performed to compare their performance against the SVM model.\n"
      ],
      "metadata": {
        "id": "rKRxQJ3FluFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6kdjD9T-3fH",
        "outputId": "18ca629d-a896-44f5-dfd6-dd7b36dd14b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=  33.1s\n",
            "[CV] END ......................C=0.1, gamma=1, kernel=linear; total time=  40.4s\n",
            "[CV] END C=37.55401188473625, gamma=0.9517143064099162, kernel=rbf; total time= 1.2min\n",
            "[CV] END C=37.55401188473625, gamma=0.9517143064099162, kernel=rbf; total time= 1.2min\n",
            "[CV] END C=37.55401188473625, gamma=0.9517143064099162, kernel=rbf; total time=  55.3s\n",
            "[CV] END C=78.06910002727692, gamma=0.597850157946487, kernel=linear; total time= 2.0min\n",
            "[CV] END C=78.06910002727692, gamma=0.597850157946487, kernel=linear; total time= 1.9min\n",
            "[CV] END C=15.699452033620265, gamma=0.05908361216819946, kernel=linear; total time=  39.2s\n",
            "[CV] END C=78.06910002727692, gamma=0.597850157946487, kernel=linear; total time= 1.9min\n",
            "[CV] END C=15.699452033620265, gamma=0.05908361216819946, kernel=linear; total time=  35.4s\n",
            "[CV] END C=15.699452033620265, gamma=0.05908361216819946, kernel=linear; total time=  35.6s\n",
            "[CV] END C=33.47086111390219, gamma=0.14386681792194078, kernel=rbf; total time=  41.4s\n",
            "[CV] END C=33.47086111390219, gamma=0.14386681792194078, kernel=rbf; total time=  41.5s\n",
            "[CV] END C=33.47086111390219, gamma=0.14386681792194078, kernel=rbf; total time=  40.7s\n",
            "[CV] END C=2.1584494295802448, gamma=0.9709098521619943, kernel=linear; total time=  18.7s\n",
            "[CV] END C=2.1584494295802448, gamma=0.9709098521619943, kernel=linear; total time=  20.0s\n",
            "[CV] END C=2.1584494295802448, gamma=0.9709098521619943, kernel=linear; total time=  21.5s\n",
            "[CV] END C=93.955270901575, gamma=0.0017787658410143284, kernel=linear; total time= 2.1min\n",
            "[CV] END C=93.955270901575, gamma=0.0017787658410143284, kernel=linear; total time= 2.6min\n",
            "[CV] END C=18.440450985343382, gamma=0.3052422429595377, kernel=linear; total time=  41.9s\n",
            "[CV] END C=18.440450985343382, gamma=0.3052422429595377, kernel=linear; total time=  41.8s\n",
            "[CV] END C=93.955270901575, gamma=0.0017787658410143284, kernel=linear; total time= 2.3min\n",
            "[CV] END C=18.440450985343382, gamma=0.3052422429595377, kernel=linear; total time=  41.9s\n",
            "[CV] END C=0.8066305219717406, gamma=0.024062425041415758, kernel=rbf; total time=  28.1s\n",
            "[CV] END C=0.8066305219717406, gamma=0.024062425041415758, kernel=rbf; total time=  28.4s\n",
            "[CV] END C=0.8066305219717406, gamma=0.024062425041415758, kernel=rbf; total time=  28.2s\n",
            "[CV] END C=61.28528947223795, gamma=0.14049386065204184, kernel=linear; total time= 1.5min\n",
            "[CV] END C=61.28528947223795, gamma=0.14049386065204184, kernel=linear; total time= 1.9min\n",
            "[CV] END C=61.28528947223795, gamma=0.14049386065204184, kernel=linear; total time= 1.5min\n",
            "[CV] END C=97.47555188414591, gamma=0.23377134043030423, kernel=linear; total time= 2.6min\n",
            "[CV] END C=97.47555188414591, gamma=0.23377134043030423, kernel=linear; total time= 2.3min\n",
            "[CV] END C=78.61759613930136, gamma=0.20067378215835974, kernel=rbf; total time=  51.3s\n",
            "[CV] END C=97.47555188414591, gamma=0.23377134043030423, kernel=linear; total time= 2.6min\n",
            "[CV] END C=78.61759613930136, gamma=0.20067378215835974, kernel=rbf; total time=  48.6s\n",
            "[CV] END C=78.61759613930136, gamma=0.20067378215835974, kernel=rbf; total time=  51.6s\n",
            "[CV] END C=98.42308858067881, gamma=0.4677628932479799, kernel=rbf; total time=  53.8s\n",
            "[CV] END C=98.42308858067881, gamma=0.4677628932479799, kernel=rbf; total time=  51.2s\n",
            "[CV] END C=98.42308858067881, gamma=0.4677628932479799, kernel=rbf; total time=  52.3s\n",
            "[CV] END C=60.85448519014384, gamma=0.17152412368729153, kernel=rbf; total time=  51.7s\n",
            "[CV] END C=60.85448519014384, gamma=0.17152412368729153, kernel=rbf; total time=  51.7s\n",
            "[CV] END C=1.426496115986653, gamma=0.9432017556848528, kernel=linear; total time=  20.3s\n",
            "[CV] END C=60.85448519014384, gamma=0.17152412368729153, kernel=rbf; total time=  55.2s\n",
            "[CV] END C=1.426496115986653, gamma=0.9432017556848528, kernel=linear; total time=  19.7s\n",
            "[CV] END C=1.426496115986653, gamma=0.9432017556848528, kernel=linear; total time=  19.8s\n",
            "[CV] END C=80.9397348116461, gamma=0.3056137691733707, kernel=rbf; total time=  54.7s\n",
            "[CV] END C=80.9397348116461, gamma=0.3056137691733707, kernel=rbf; total time=  53.0s\n",
            "[CV] END C=80.9397348116461, gamma=0.3056137691733707, kernel=rbf; total time=  53.7s\n",
            "[CV] END C=23.189382562214902, gamma=0.24202546602601172, kernel=linear; total time=  45.3s\n",
            "[CV] END C=23.189382562214902, gamma=0.24202546602601172, kernel=linear; total time=  47.8s\n",
            "[CV] END C=23.189382562214902, gamma=0.24202546602601172, kernel=linear; total time=  48.9s\n",
            "[CV] END C=12.303823484477883, gamma=0.4961769101112702, kernel=rbf; total time=  58.8s\n",
            "[CV] END C=12.303823484477883, gamma=0.4961769101112702, kernel=rbf; total time=  58.2s\n",
            "[CV] END C=17.43646535077721, gamma=0.3920606075732408, kernel=linear; total time=  42.1s\n",
            "[CV] END C=12.303823484477883, gamma=0.4961769101112702, kernel=rbf; total time=  55.9s\n",
            "[CV] END C=17.43646535077721, gamma=0.3920606075732408, kernel=linear; total time=  43.2s\n",
            "[CV] END C=17.43646535077721, gamma=0.3920606075732408, kernel=linear; total time=  40.8s\n",
            "[CV] END C=66.35222843539819, gamma=0.31271107608941096, kernel=linear; total time= 1.8min\n",
            "[CV] END C=66.35222843539819, gamma=0.31271107608941096, kernel=linear; total time= 2.0min\n",
            "[CV] END C=20.894166286818884, gamma=0.5687003278199915, kernel=rbf; total time=  57.7s\n",
            "[CV] END C=66.35222843539819, gamma=0.31271107608941096, kernel=linear; total time= 1.8min\n",
            "[CV] END C=20.894166286818884, gamma=0.5687003278199915, kernel=rbf; total time=  57.6s\n",
            "[CV] END C=20.894166286818884, gamma=0.5687003278199915, kernel=rbf; total time=  41.0s\n",
            "Best Model: SVC(C=37.55401188473625, gamma=0.9517143064099162, random_state=42)\n",
            "Accuracy: 0.8105167724388033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.81      0.81      2189\n",
            "           1       0.81      0.81      0.81      2223\n",
            "\n",
            "    accuracy                           0.81      4412\n",
            "   macro avg       0.81      0.81      0.81      4412\n",
            "weighted avg       0.81      0.81      0.81      4412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Function to perform hyperparameter tuning and evaluate models\n",
        "def tune_and_evaluate_model(X_train, X_test, y_train, y_test, model, param_grid):\n",
        "    with joblib.parallel_backend('threading'):\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    print(f\"Best Model: {best_model}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# SVM Hyperparameter Tuning\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "svm_best_model = tune_and_evaluate_model(X_train, X_test, y_train, y_test, SVC(random_state=42), svm_param_grid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Hyperparameter Tuning\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "xgb_best_model = tune_and_evaluate_model(X_train, X_test, y_train, y_test, XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'), xgb_param_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz-TVISNdnc3",
        "outputId": "8cc42bd8-314d-47f0-ce8d-bb9b681efbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   7.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   7.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   4.9s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=   8.9s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  11.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  11.9s\n",
            "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   6.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   7.0s\n",
            "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   7.1s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  17.0s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  16.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=200; total time=  11.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  14.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=200; total time=   7.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=200; total time=   8.8s\n",
            "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   6.7s\n",
            "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   6.0s\n",
            "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=   4.9s\n",
            "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   4.6s\n",
            "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   5.5s\n",
            "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   6.5s\n",
            "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   7.8s\n",
            "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   6.8s\n",
            "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=   8.6s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  11.0s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=   9.1s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=100; total time=   7.9s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  13.0s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=100; total time=   4.3s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=100; total time=   4.3s\n",
            "Best Model: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eval_metric='mlogloss',\n",
            "              feature_types=None, gamma=None, grow_policy=None,\n",
            "              importance_type=None, interaction_constraints=None,\n",
            "              learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
            "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
            "              max_leaves=None, min_child_weight=None, missing=nan,\n",
            "              monotone_constraints=None, multi_strategy=None, n_estimators=200,\n",
            "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)\n",
            "Accuracy: 0.7318676337262012\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.86      0.76      2189\n",
            "           1       0.82      0.60      0.69      2223\n",
            "\n",
            "    accuracy                           0.73      4412\n",
            "   macro avg       0.75      0.73      0.73      4412\n",
            "weighted avg       0.75      0.73      0.73      4412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Hyperparameter Tuning\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 4, 5]\n",
        "}\n",
        "gb_best_model = tune_and_evaluate_model(X_train, X_test, y_train, y_test, GradientBoostingClassifier(random_state=42), gb_param_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJva7ZbmeY2c",
        "outputId": "8905562a-95ac-4d68-ea83-5ad6708c61db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=  10.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=  11.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=100; total time=   7.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  16.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  14.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=3, n_estimators=200; total time=  12.4s\n",
            "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   8.5s\n",
            "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   6.8s\n",
            "[CV] END ...learning_rate=0.1, max_depth=4, n_estimators=100; total time=   6.7s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  19.6s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  19.6s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=200; total time=  14.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=200; total time=  17.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=200; total time=  15.8s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=200; total time=  15.7s\n",
            "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.5s\n",
            "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  12.6s\n",
            "[CV] END ...learning_rate=0.2, max_depth=3, n_estimators=200; total time=  11.8s\n",
            "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   9.8s\n",
            "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   8.4s\n",
            "[CV] END ...learning_rate=0.2, max_depth=5, n_estimators=100; total time=   7.9s\n",
            "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=  17.0s\n",
            "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=  17.4s\n",
            "[CV] END ...learning_rate=0.2, max_depth=4, n_estimators=200; total time=  14.7s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  19.3s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  20.0s\n",
            "[CV] END ...learning_rate=0.1, max_depth=5, n_estimators=200; total time=  17.9s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=100; total time=   6.5s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=100; total time=   7.2s\n",
            "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=100; total time=   6.7s\n",
            "Best Model: GradientBoostingClassifier(learning_rate=0.2, max_depth=4, n_estimators=200,\n",
            "                           random_state=42)\n",
            "Accuracy: 0.7411604714415231\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.85      0.77      2189\n",
            "           1       0.81      0.63      0.71      2223\n",
            "\n",
            "    accuracy                           0.74      4412\n",
            "   macro avg       0.75      0.74      0.74      4412\n",
            "weighted avg       0.75      0.74      0.74      4412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Learning Model Training and Evaluation\n",
        "\n",
        "The provided output shows the training progress and evaluation metrics of a deep learning model trained over 10 epochs.\n",
        "\n",
        "#### Training Progress:\n",
        "\n",
        "- **Epochs:** The model is trained over 10 epochs.\n",
        "- **Loss:** The loss decreases from 0.6915 to 0.3594, indicating improvement in model performance.\n",
        "- **Accuracy:** The accuracy improves from 0.5304 to 0.8388 on the training set.\n",
        "- **Validation Accuracy:** The validation accuracy increases from 0.5971 to 0.7121, showing that the model generalizes reasonably well to unseen data but may not perform as well as on the training data.\n",
        "\n",
        "#### Model Evaluation:\n",
        "\n",
        "- After training, the model achieves an accuracy of 0.7246 on the test set (`val_accuracy`).\n",
        "- The `classification_report` can provide more insights into precision, recall, and F1-score for each class (not shown here).\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- The deep learning model shows a promising improvement in accuracy and loss during training.\n",
        "- However, there might be some overfitting as the validation accuracy is slightly lower than the training accuracy.\n",
        "- Further tuning of hyperparameters or model architecture adjustments may be beneficial to improve validation performance and overall model robustness.\n"
      ],
      "metadata": {
        "id": "_oRvx7SnmF4e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIcoei_Gh89x",
        "outputId": "f78342d2-b27a-4942-8870-14447b9f7994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "442/442 [==============================] - 3s 5ms/step - loss: 0.6915 - accuracy: 0.5304 - val_loss: 0.6860 - val_accuracy: 0.5971\n",
            "Epoch 2/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.6515 - accuracy: 0.6704 - val_loss: 0.6122 - val_accuracy: 0.6971\n",
            "Epoch 3/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.5398 - accuracy: 0.7482 - val_loss: 0.5504 - val_accuracy: 0.7078\n",
            "Epoch 4/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.4701 - accuracy: 0.7807 - val_loss: 0.5386 - val_accuracy: 0.7226\n",
            "Epoch 5/10\n",
            "442/442 [==============================] - 3s 6ms/step - loss: 0.4370 - accuracy: 0.7957 - val_loss: 0.5518 - val_accuracy: 0.7149\n",
            "Epoch 6/10\n",
            "442/442 [==============================] - 3s 6ms/step - loss: 0.4128 - accuracy: 0.8094 - val_loss: 0.5580 - val_accuracy: 0.7183\n",
            "Epoch 7/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3960 - accuracy: 0.8204 - val_loss: 0.5565 - val_accuracy: 0.7226\n",
            "Epoch 8/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3803 - accuracy: 0.8262 - val_loss: 0.5718 - val_accuracy: 0.7200\n",
            "Epoch 9/10\n",
            "442/442 [==============================] - 2s 5ms/step - loss: 0.3673 - accuracy: 0.8319 - val_loss: 0.5903 - val_accuracy: 0.7147\n",
            "Epoch 10/10\n",
            "442/442 [==============================] - 2s 5ms/step - loss: 0.3594 - accuracy: 0.8388 - val_loss: 0.5954 - val_accuracy: 0.7121\n",
            "Deep Learning Model Accuracy: 0.724614679813385\n"
          ]
        }
      ],
      "source": [
        "# For Deep Learning model\n",
        "dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwyd7sOimNvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfHHqMYAmNsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S2T0a_CvmNqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HyFFYOXmNnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKZr6moYmNlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPPJRNkOmNjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wdw5vHTtmNRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z2h1QZqhWIl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlU0FSk9hWTP"
      },
      "source": [
        "Performing Varoius practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G0FysPSZmWu",
        "outputId": "a6c99830-8322-45d5-caa2-75891544caeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: random_forest\n",
            "Accuracy: 0.7660924750679964\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77      2189\n",
            "           1       0.78      0.75      0.76      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: logistic_regression\n",
            "Accuracy: 0.772438803263826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.77      0.77      2189\n",
            "           1       0.78      0.77      0.77      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: svm\n",
            "Accuracy: 0.8125566636446057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82      2189\n",
            "           1       0.84      0.78      0.81      2223\n",
            "\n",
            "    accuracy                           0.81      4412\n",
            "   macro avg       0.81      0.81      0.81      4412\n",
            "weighted avg       0.81      0.81      0.81      4412\n",
            "\n",
            "Model: gradient_boosting\n",
            "Accuracy: 0.6860834088848595\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.90      0.74      2189\n",
            "           1       0.83      0.48      0.60      2223\n",
            "\n",
            "    accuracy                           0.69      4412\n",
            "   macro avg       0.73      0.69      0.67      4412\n",
            "weighted avg       0.73      0.69      0.67      4412\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest'):\n",
        "    if model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "    elif model_type == 'logistic_regression':\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    elif model_type == 'svm':\n",
        "        model = SVC(random_state=42)\n",
        "    elif model_type == 'gradient_boosting':\n",
        "        model = GradientBoostingClassifier(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type. Choose from 'random_forest', 'logistic_regression', 'svm', or 'gradient_boosting'.\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Model: {model_type}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "rf_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='random_forest')\n",
        "lr_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='logistic_regression')\n",
        "svm_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='svm')\n",
        "gb_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='gradient_boosting')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CxiVWapZ-1O",
        "outputId": "073193c2-e19a-480a-f78a-05dcf11b4971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "442/442 [==============================] - 11s 13ms/step - loss: 0.6923 - accuracy: 0.5257 - val_loss: 0.6903 - val_accuracy: 0.5180\n",
            "Epoch 2/10\n",
            "442/442 [==============================] - 7s 15ms/step - loss: 0.6752 - accuracy: 0.6448 - val_loss: 0.6504 - val_accuracy: 0.6747\n",
            "Epoch 3/10\n",
            "442/442 [==============================] - 6s 13ms/step - loss: 0.5900 - accuracy: 0.7384 - val_loss: 0.5731 - val_accuracy: 0.7161\n",
            "Epoch 4/10\n",
            "442/442 [==============================] - 6s 14ms/step - loss: 0.5063 - accuracy: 0.7723 - val_loss: 0.5455 - val_accuracy: 0.7192\n",
            "Epoch 5/10\n",
            "442/442 [==============================] - 7s 16ms/step - loss: 0.4623 - accuracy: 0.7873 - val_loss: 0.5388 - val_accuracy: 0.7226\n",
            "Epoch 6/10\n",
            "442/442 [==============================] - 5s 12ms/step - loss: 0.4357 - accuracy: 0.8004 - val_loss: 0.5498 - val_accuracy: 0.7129\n",
            "Epoch 7/10\n",
            "442/442 [==============================] - 4s 9ms/step - loss: 0.4168 - accuracy: 0.8064 - val_loss: 0.5441 - val_accuracy: 0.7260\n",
            "Epoch 8/10\n",
            "442/442 [==============================] - 3s 6ms/step - loss: 0.4013 - accuracy: 0.8157 - val_loss: 0.5515 - val_accuracy: 0.7232\n",
            "Epoch 9/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3873 - accuracy: 0.8237 - val_loss: 0.5640 - val_accuracy: 0.7169\n",
            "Epoch 10/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3771 - accuracy: 0.8299 - val_loss: 0.5680 - val_accuracy: 0.7212\n",
            "Deep Learning Model Accuracy: 0.7441070079803467\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "def build_and_evaluate_dl_model(X_train, X_test, y_train, y_test):\n",
        "    # Convert target to categorical if necessary\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=16, input_length=maxlen),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with class weights\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weights, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Deep Learning Model Accuracy: {accuracy}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv_UCvkBcWQI",
        "outputId": "de3961b2-00a0-4734-e568-d1c8655764ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "Deep Learning Model Accuracy: 0.5\n",
            "Deep Learning Model Precision: 0.5\n",
            "Deep Learning Model Recall: 1.0\n",
            "Deep Learning Model F1 Score: 0.6666666865348816\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Function for WordPiece tokenization\n",
        "def wordpiece_tokenize(texts, tokenizer, max_length):\n",
        "    tokenized_texts = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='np'\n",
        "    )\n",
        "    return tokenized_texts['input_ids']\n",
        "\n",
        "def build_and_evaluate_dl_model(X_train, X_test, y_train, y_test, maxlen=100):\n",
        "    # Convert target to categorical if necessary\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Ensure input data is in list of strings format\n",
        "    X_train = [str(doc) for doc in X_train]\n",
        "    X_test = [str(doc) for doc in X_test]\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    # WordPiece Tokenization\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    X_train_tokenized = wordpiece_tokenize(X_train, tokenizer, max_length=maxlen)\n",
        "    X_test_tokenized = wordpiece_tokenize(X_test, tokenizer, max_length=maxlen)\n",
        "\n",
        "    # Combine TF-IDF and Tokenized features (you may choose to use one or both)\n",
        "    X_train_combined = np.hstack((X_train_tfidf, X_train_tokenized))\n",
        "    X_test_combined = np.hstack((X_test_tfidf, X_test_tokenized))\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with class weights\n",
        "    model.fit(X_train_combined, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weights, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = (model.predict(X_test_combined) > 0.5).astype(\"int32\")\n",
        "    accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "    precision = tf.keras.metrics.Precision()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    f1_score = tf.keras.metrics.Mean()\n",
        "\n",
        "    accuracy.update_state(y_test, y_pred)\n",
        "    precision.update_state(y_test, y_pred)\n",
        "    recall.update_state(y_test, y_pred)\n",
        "    f1 = 2 * (precision.result().numpy() * recall.result().numpy()) / (precision.result().numpy() + recall.result().numpy())\n",
        "    f1_score.update_state(f1)\n",
        "\n",
        "    print(f\"Deep Learning Model Accuracy: {accuracy.result().numpy()}\")\n",
        "    print(f\"Deep Learning Model Precision: {precision.result().numpy()}\")\n",
        "    print(f\"Deep Learning Model Recall: {recall.result().numpy()}\")\n",
        "    print(f\"Deep Learning Model F1 Score: {f1_score.result().numpy()}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "# Ensure X_train_dl and X_test_dl are lists of strings, and y_train_dl and y_test_dl are labels\n",
        "dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gPy4FWqfIPo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}