{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Data"
      ],
      "metadata": {
        "id": "t5va_KOkfjR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L-A7DHwUJgn",
        "outputId": "5e8f43ee-5189-46e1-b6ab-e2b211553814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/preprocessed_dataset.csv')\n",
        "\n",
        "# Define the text and target columns\n",
        "text_column = 'cleaned_comment'\n",
        "target_column = 'labels'\n"
      ],
      "metadata": {
        "id": "YDnklkmoU7Kz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the Text Data"
      ],
      "metadata": {
        "id": "hnAjQxA5f-yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS7GOnw8Vy3s",
        "outputId": "8ce9be27-cc8c-445f-81f5-2b7338e9a971"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the stopwords resource\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a function to clean and preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove punctuation and digits\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Join tokens back into a single string\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "# Apply text preprocessing\n",
        "df[text_column] = df[text_column].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Jxd2OCVFgE",
        "outputId": "7d328f65-bfa9-4a8a-bfa8-c8ca72d245bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize the Text Data Using TF-IDF"
      ],
      "metadata": {
        "id": "SbJURwCFgEuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf_vectorizer.fit_transform(df[text_column])\n",
        "y = df[target_column]\n"
      ],
      "metadata": {
        "id": "EnagGdzlVKP0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Imbalanced Data for Machine Learning Models"
      ],
      "metadata": {
        "id": "bssDE1zZgaRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Apply SMOTE to the TF-IDF vectorized data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "NqPSCBJRggDJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Function to Build and Evaluate Machine Learning Models"
      ],
      "metadata": {
        "id": "4uh-SGGxgiyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest'):\n",
        "    if model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "    elif model_type == 'logistic_regression':\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    elif model_type == 'svm':\n",
        "        model = SVC(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type. Choose from 'random_forest', 'logistic_regression', or 'svm'.\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Model: {model_type}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model\n",
        "\n",
        "# # Example usage\n",
        "# rf_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='random_forest')\n",
        "# lr_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='logistic_regression')\n",
        "# svm_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='svm')\n"
      ],
      "metadata": {
        "id": "YdKhKoXUWOhw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Imbalanced Data for Deep Learning Models\n",
        " Tokenize and Pad the Text Dat"
      ],
      "metadata": {
        "id": "LTB4bdjUgtXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# # Tokenize the text data\n",
        "# tokenizer = Tokenizer(num_words=5000)\n",
        "# tokenizer.fit_on_texts(df[text_column])\n",
        "# X_tokenized = tokenizer.texts_to_sequences(df[text_column])\n",
        "\n",
        "# # Pad the sequences\n",
        "# maxlen = 100\n",
        "# X_padded = pad_sequences(X_tokenized, padding='post', maxlen=maxlen)\n",
        "\n",
        "# # Apply SMOTE to the padded sequences\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_resampled_dl, y_resampled_dl = smote.fit_resample(X_padded, df[target_column])\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_resampled_dl, y_resampled_dl, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "2jY1NBfFX38Y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[text_column])\n",
        "y = df[target_column]\n",
        "\n",
        "# Apply SMOTE to the data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "FDb8DJE4WJjx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to Build and Evaluate Deep Learning Models"
      ],
      "metadata": {
        "id": "lsuSpf0RhFay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def build_and_evaluate_dl_model(X_train, X_test, y_train, y_test):\n",
        "    # Convert target to categorical if necessary\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=16, input_length=maxlen),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Deep Learning Model Accuracy: {accuracy}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "#dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ],
      "metadata": {
        "id": "1Wc5nCdcWSIH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Function Calls to Generate Models"
      ],
      "metadata": {
        "id": "j_YL6czyhM9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Machine Learning models\n",
        "rf_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest')\n",
        "lr_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='logistic_regression')\n",
        "svm_model = build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='svm')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdgJntSGWbhK",
        "outputId": "54b14a99-2025-4b5c-dce2-ef9ede5acd5d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: random_forest\n",
            "Accuracy: 0.7660924750679964\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77      2189\n",
            "           1       0.78      0.75      0.76      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: logistic_regression\n",
            "Accuracy: 0.772438803263826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.77      0.77      2189\n",
            "           1       0.78      0.77      0.77      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: svm\n",
            "Accuracy: 0.8125566636446057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82      2189\n",
            "           1       0.84      0.78      0.81      2223\n",
            "\n",
            "    accuracy                           0.81      4412\n",
            "   macro avg       0.81      0.81      0.81      4412\n",
            "weighted avg       0.81      0.81      0.81      4412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Deep Learning model\n",
        "dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIcoei_Gh89x",
        "outputId": "70233b5d-ca01-437d-f448-0e584e711fc2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "442/442 [==============================] - 5s 7ms/step - loss: 0.6919 - accuracy: 0.5267 - val_loss: 0.6882 - val_accuracy: 0.5137\n",
            "Epoch 2/10\n",
            "442/442 [==============================] - 3s 8ms/step - loss: 0.6549 - accuracy: 0.6481 - val_loss: 0.6155 - val_accuracy: 0.6600\n",
            "Epoch 3/10\n",
            "442/442 [==============================] - 4s 9ms/step - loss: 0.5386 - accuracy: 0.7511 - val_loss: 0.5655 - val_accuracy: 0.6832\n",
            "Epoch 4/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.4705 - accuracy: 0.7785 - val_loss: 0.5302 - val_accuracy: 0.7186\n",
            "Epoch 5/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.4389 - accuracy: 0.7945 - val_loss: 0.5337 - val_accuracy: 0.7217\n",
            "Epoch 6/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.4176 - accuracy: 0.8043 - val_loss: 0.5308 - val_accuracy: 0.7243\n",
            "Epoch 7/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.4012 - accuracy: 0.8163 - val_loss: 0.5394 - val_accuracy: 0.7246\n",
            "Epoch 8/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3849 - accuracy: 0.8258 - val_loss: 0.5571 - val_accuracy: 0.7223\n",
            "Epoch 9/10\n",
            "442/442 [==============================] - 2s 5ms/step - loss: 0.3706 - accuracy: 0.8332 - val_loss: 0.5627 - val_accuracy: 0.7237\n",
            "Epoch 10/10\n",
            "442/442 [==============================] - 3s 7ms/step - loss: 0.3639 - accuracy: 0.8340 - val_loss: 0.5877 - val_accuracy: 0.7195\n",
            "Deep Learning Model Accuracy: 0.7436536550521851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5z2h1QZqhWIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Varoius practices"
      ],
      "metadata": {
        "id": "LlU0FSk9hWTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def build_and_evaluate_ml_model(X_train, X_test, y_train, y_test, model_type='random_forest'):\n",
        "    if model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "    elif model_type == 'logistic_regression':\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    elif model_type == 'svm':\n",
        "        model = SVC(random_state=42)\n",
        "    elif model_type == 'gradient_boosting':\n",
        "        model = GradientBoostingClassifier(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model type. Choose from 'random_forest', 'logistic_regression', 'svm', or 'gradient_boosting'.\")\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"Model: {model_type}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "rf_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='random_forest')\n",
        "lr_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='logistic_regression')\n",
        "svm_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='svm')\n",
        "gb_model = build_and_evaluate_ml_model(X_train_ml, X_test_ml, y_train_ml, y_test_ml, model_type='gradient_boosting')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G0FysPSZmWu",
        "outputId": "a6c99830-8322-45d5-caa2-75891544caeb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: random_forest\n",
            "Accuracy: 0.7660924750679964\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.77      2189\n",
            "           1       0.78      0.75      0.76      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: logistic_regression\n",
            "Accuracy: 0.772438803263826\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.77      0.77      2189\n",
            "           1       0.78      0.77      0.77      2223\n",
            "\n",
            "    accuracy                           0.77      4412\n",
            "   macro avg       0.77      0.77      0.77      4412\n",
            "weighted avg       0.77      0.77      0.77      4412\n",
            "\n",
            "Model: svm\n",
            "Accuracy: 0.8125566636446057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82      2189\n",
            "           1       0.84      0.78      0.81      2223\n",
            "\n",
            "    accuracy                           0.81      4412\n",
            "   macro avg       0.81      0.81      0.81      4412\n",
            "weighted avg       0.81      0.81      0.81      4412\n",
            "\n",
            "Model: gradient_boosting\n",
            "Accuracy: 0.6860834088848595\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.90      0.74      2189\n",
            "           1       0.83      0.48      0.60      2223\n",
            "\n",
            "    accuracy                           0.69      4412\n",
            "   macro avg       0.73      0.69      0.67      4412\n",
            "weighted avg       0.73      0.69      0.67      4412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "def build_and_evaluate_dl_model(X_train, X_test, y_train, y_test):\n",
        "    # Convert target to categorical if necessary\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=16, input_length=maxlen),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with class weights\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weights, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Deep Learning Model Accuracy: {accuracy}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CxiVWapZ-1O",
        "outputId": "073193c2-e19a-480a-f78a-05dcf11b4971"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "442/442 [==============================] - 11s 13ms/step - loss: 0.6923 - accuracy: 0.5257 - val_loss: 0.6903 - val_accuracy: 0.5180\n",
            "Epoch 2/10\n",
            "442/442 [==============================] - 7s 15ms/step - loss: 0.6752 - accuracy: 0.6448 - val_loss: 0.6504 - val_accuracy: 0.6747\n",
            "Epoch 3/10\n",
            "442/442 [==============================] - 6s 13ms/step - loss: 0.5900 - accuracy: 0.7384 - val_loss: 0.5731 - val_accuracy: 0.7161\n",
            "Epoch 4/10\n",
            "442/442 [==============================] - 6s 14ms/step - loss: 0.5063 - accuracy: 0.7723 - val_loss: 0.5455 - val_accuracy: 0.7192\n",
            "Epoch 5/10\n",
            "442/442 [==============================] - 7s 16ms/step - loss: 0.4623 - accuracy: 0.7873 - val_loss: 0.5388 - val_accuracy: 0.7226\n",
            "Epoch 6/10\n",
            "442/442 [==============================] - 5s 12ms/step - loss: 0.4357 - accuracy: 0.8004 - val_loss: 0.5498 - val_accuracy: 0.7129\n",
            "Epoch 7/10\n",
            "442/442 [==============================] - 4s 9ms/step - loss: 0.4168 - accuracy: 0.8064 - val_loss: 0.5441 - val_accuracy: 0.7260\n",
            "Epoch 8/10\n",
            "442/442 [==============================] - 3s 6ms/step - loss: 0.4013 - accuracy: 0.8157 - val_loss: 0.5515 - val_accuracy: 0.7232\n",
            "Epoch 9/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3873 - accuracy: 0.8237 - val_loss: 0.5640 - val_accuracy: 0.7169\n",
            "Epoch 10/10\n",
            "442/442 [==============================] - 2s 4ms/step - loss: 0.3771 - accuracy: 0.8299 - val_loss: 0.5680 - val_accuracy: 0.7212\n",
            "Deep Learning Model Accuracy: 0.7441070079803467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Function for WordPiece tokenization\n",
        "def wordpiece_tokenize(texts, tokenizer, max_length):\n",
        "    tokenized_texts = tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='np'\n",
        "    )\n",
        "    return tokenized_texts['input_ids']\n",
        "\n",
        "def build_and_evaluate_dl_model(X_train, X_test, y_train, y_test, maxlen=100):\n",
        "    # Convert target to categorical if necessary\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Ensure input data is in list of strings format\n",
        "    X_train = [str(doc) for doc in X_train]\n",
        "    X_test = [str(doc) for doc in X_test]\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
        "\n",
        "    # WordPiece Tokenization\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    X_train_tokenized = wordpiece_tokenize(X_train, tokenizer, max_length=maxlen)\n",
        "    X_test_tokenized = wordpiece_tokenize(X_test, tokenizer, max_length=maxlen)\n",
        "\n",
        "    # Combine TF-IDF and Tokenized features (you may choose to use one or both)\n",
        "    X_train_combined = np.hstack((X_train_tfidf, X_train_tokenized))\n",
        "    X_test_combined = np.hstack((X_test_tfidf, X_test_tokenized))\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(X_train_combined.shape[1],)),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with class weights\n",
        "    model.fit(X_train_combined, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weights, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = (model.predict(X_test_combined) > 0.5).astype(\"int32\")\n",
        "    accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "    precision = tf.keras.metrics.Precision()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    f1_score = tf.keras.metrics.Mean()\n",
        "\n",
        "    accuracy.update_state(y_test, y_pred)\n",
        "    precision.update_state(y_test, y_pred)\n",
        "    recall.update_state(y_test, y_pred)\n",
        "    f1 = 2 * (precision.result().numpy() * recall.result().numpy()) / (precision.result().numpy() + recall.result().numpy())\n",
        "    f1_score.update_state(f1)\n",
        "\n",
        "    print(f\"Deep Learning Model Accuracy: {accuracy.result().numpy()}\")\n",
        "    print(f\"Deep Learning Model Precision: {precision.result().numpy()}\")\n",
        "    print(f\"Deep Learning Model Recall: {recall.result().numpy()}\")\n",
        "    print(f\"Deep Learning Model F1 Score: {f1_score.result().numpy()}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "# Ensure X_train_dl and X_test_dl are lists of strings, and y_train_dl and y_test_dl are labels\n",
        "dl_model = build_and_evaluate_dl_model(X_train_dl, X_test_dl, y_train_dl, y_test_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv_UCvkBcWQI",
        "outputId": "de3961b2-00a0-4734-e568-d1c8655764ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "Deep Learning Model Accuracy: 0.5\n",
            "Deep Learning Model Precision: 0.5\n",
            "Deep Learning Model Recall: 1.0\n",
            "Deep Learning Model F1 Score: 0.6666666865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_gPy4FWqfIPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}