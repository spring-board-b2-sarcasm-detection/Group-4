{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd99fb7-3520-4bd7-9f35-85b193a841da",
   "metadata": {},
   "source": [
    "## Different Tokenizations\n",
    "\n",
    "This code demonstrates a process for preprocessing text data, performing different tokenization techniques, and evaluating a RandomForestClassifier model using TF-IDF features.\n",
    "\n",
    "#### Steps and Components:\n",
    "\n",
    "1. **Loading the Dataset:**\n",
    "   - The dataset (`preprocessed_dataset.csv`) is loaded using pandas (`pd.read_csv`). It contains columns 'cleaned_comment' (text data) and 'labels'.\n",
    "\n",
    "2. **Tokenization Functions:**\n",
    "   - Several tokenization functions are defined:\n",
    "     - `whitespace_tokenize`: Splits text based on whitespace.\n",
    "     - `punctuation_tokenize`: Extracts words and punctuation marks using regular expressions.\n",
    "     - `ngram_tokenize`: Generates n-grams (in this case, bi-grams) from the text.\n",
    "     - `wordpiece_tokenize`: Uses BERT's tokenizer (`BertTokenizer`) to tokenize text into WordPieces.\n",
    "     - `sentencepiece_tokenize`: Utilizes SentencePiece for tokenization. Training data (`text.txt`) is prepared and trained with SentencePiece for segmentation into subword units.\n",
    "     - `bpe_tokenize`: Utilizes Byte Pair Encoding (BPE) tokenization. Training data (`text.txt`) is prepared and trained with BPE to create a vocabulary of subword units.\n",
    "\n",
    "3. **Training SentencePiece and BPE Tokenizers:**\n",
    "   - `spm.SentencePieceTrainer.Train()` trains a SentencePiece model with specified parameters (`--input=text.txt --model_prefix=m --vocab_size=5000`).\n",
    "   - `Tokenizer.train()` trains a BPE tokenizer (`BpeTrainer`) with `vocab_size=5000` and `min_frequency=2`.\n",
    "\n",
    "4. **Model Training and Evaluation:**\n",
    "   - A function `train_and_evaluate_model()` is defined to:\n",
    "     - Split the data into training and testing sets using `train_test_split()`.\n",
    "     - Vectorize text data using `TfidfVectorizer()` to convert text into numerical features (TF-IDF vectors).\n",
    "     - Train a `RandomForestClassifier` model with 100 estimators.\n",
    "     - Evaluate the model's accuracy on the test set using `accuracy_score`.\n",
    "\n",
    "5. **Tokenization Methods Evaluation:**\n",
    "   - The accuracy of the RandomForestClassifier model is evaluated using different tokenization methods:\n",
    "     - `whitespace`, `punctuation`, `ngram`, `wordpiece`, `sentencepiece`, and `bpe`.\n",
    "   - For each method, text data (`X`) is tokenized using the corresponding tokenization function, and the accuracy of the trained model is printed.\n",
    "\n",
    "#### Suggestions for Improvement:\n",
    "\n",
    "- **Error Handling:** Implement error handling to manage potential issues such as file not found errors or tokenization failures.\n",
    "- **Visualization:** Include visualizations such as confusion matrices to better understand model performance.\n",
    "- **Parameter Tuning:** Explore tuning parameters for tokenizers and the classifier to potentially improve model accuracy.\n",
    "- **Scaling:** Consider scaling up to larger datasets or optimizing code for efficiency.\n",
    "\n",
    "This approach provides a comprehensive example of text preprocessing, tokenization using different methods, and evaluation of a machine learning model, showcasing the versatility of tokenization techniques in natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a469cd13-ce3c-47f9-90ea-45b88eba2641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d67f063c94daa99062375410c72e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\manis\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d22291fa20f4e15ab464e22cfe72060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bba1e9c48154d6091533d26e8a04561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manis\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7585eeff05b94557b46296e61201742f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitespace tokenization - Model Accuracy: 0.7412461380020597\n",
      "punctuation tokenization - Model Accuracy: 0.7412461380020597\n",
      "ngram tokenization - Model Accuracy: 0.7438208032955715\n",
      "wordpiece tokenization - Model Accuracy: 0.7492276004119465\n",
      "sentencepiece tokenization - Model Accuracy: 0.7440782698249228\n",
      "bpe tokenization - Model Accuracy: 0.7438208032955715\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'M:\\\\Internships\\\\infosys_springboard\\\\Notebooks\\\\Preprocessing\\\\preprocessed_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming the dataset has columns 'cleaned_comment' and 'labels'\n",
    "X = df['cleaned_comment']\n",
    "y = df['labels']\n",
    "\n",
    "# Define tokenization functions\n",
    "def whitespace_tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def punctuation_tokenize(text):\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "\n",
    "def ngram_tokenize(text, n):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "# WordPiece Tokenization using BERT's tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def wordpiece_tokenize(text):\n",
    "    return bert_tokenizer.tokenize(text)\n",
    "\n",
    "# SentencePiece Tokenization\n",
    "# Ensure to have a text file for training SentencePiece\n",
    "# Here we assume 'text.txt' exists and contains relevant training data\n",
    "with open('text.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in X:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=text.txt --model_prefix=m --vocab_size=5000')\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "\n",
    "def sentencepiece_tokenize(text):\n",
    "    return sp.encode_as_pieces(text)\n",
    "\n",
    "# Byte Pair Encoding (BPE)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=5000, min_frequency=2)\n",
    "tokenizer.train(files=['text.txt'], trainer=trainer)\n",
    "tokenizer.save(\"bpe_tokenizer.json\")\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "def bpe_tokenize(text):\n",
    "    return tokenizer.encode(text).tokens\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Apply tokenization methods\n",
    "tokenization_methods = {\n",
    "    'whitespace': whitespace_tokenize,\n",
    "    'punctuation': punctuation_tokenize,\n",
    "    'ngram': lambda text: ngram_tokenize(text, 2),\n",
    "    'wordpiece': wordpiece_tokenize,\n",
    "    'sentencepiece': sentencepiece_tokenize,\n",
    "    'bpe': bpe_tokenize\n",
    "}\n",
    "\n",
    "for method_name, tokenize_fn in tokenization_methods.items():\n",
    "    tokenized_X = X.apply(lambda text: ' '.join(tokenize_fn(text)))\n",
    "    accuracy = train_and_evaluate_model(tokenized_X, y)\n",
    "    print(f\"{method_name} tokenization - Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24222a85-6c7a-4089-aa59-d4ab0984d772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
